<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Articles on by Alex Code blog</title><link>https://byalexblog.net/article/</link><description>Recent content in Articles on by Alex Code blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sat, 05 Apr 2025 06:39:47 +0000</lastBuildDate><atom:link href="https://byalexblog.net/article/index.xml" rel="self" type="application/rss+xml"/><item><title>Automate Power BI On-Premises Data Gateway (Self-Hosted) on Azure VM</title><link>https://byalexblog.net/article/powerbi-on-premises-data-gateway-azure-automation/</link><pubDate>Sat, 05 Apr 2025 06:39:47 +0000</pubDate><guid>https://byalexblog.net/article/powerbi-on-premises-data-gateway-azure-automation/</guid><description>&lt;img src="https://byalexblog.net/images/powerbi-on-premises-data-gateway-azure-automation/gateway-connectivity.png" alt=""> Microsoft Power BI Pro (the cloud service) often needs to query data that resides in secure, private networks. In our scenario, the data source is an Azure SQL Database deployed inside a private Azure Virtual Network with no public internet access. By default, Power BI’s cloud service cannot reach such isolated data sources directly​. In this article, we will focus on the first solution, which is more flexible and allows for a wider range of data sources.</description></item><item><title>Avoid Data Loss When Updating the AzureRM Terraform Provider</title><link>https://byalexblog.net/article/terraform-4.9.0-azurerm-storage-container-update/</link><pubDate>Mon, 25 Nov 2024 21:19:21 +0000</pubDate><guid>https://byalexblog.net/article/terraform-4.9.0-azurerm-storage-container-update/</guid><description>&lt;img src="https://byalexblog.net/images/terraform-4.9.0-azurerm-storage-container-update/title.webp" alt=""> With the release of &lt;a href="https://registry.terraform.io/providers/hashicorp/azurerm/4.9.0/docs/resources/storage_container" target="_blank" rel="noopener">AzureRM provider 4.9.0&lt;/a>, the azurerm_storage_container resource deprecates the storage_account_name argument in favor of the storage_account_id argument. Updating your Terraform template to use storage_account_id directly will force Terraform to recreate the storage container, leading to potential data loss.
Here’s a step-by-step guide to safely update your configuration without losing data in the storage container.</description></item><item><title>Azure Function (Python) Application Insights</title><link>https://byalexblog.net/article/azure-function-python-application-insights/</link><pubDate>Sat, 05 Oct 2024 11:32:19 +0000</pubDate><guid>https://byalexblog.net/article/azure-function-python-application-insights/</guid><description>I have some Python Azure Functions and recently I decided to review and improve the logging mechanism for them (Azure Application Insights integration). What I wanted to acheave is to have visual spans for individual function calls that can be nested and give the better idea of the function call behavior.
&lt;img src="https://byalexblog.net/images/azure-function-python-application-insights/application-insights-spans.png" alt=""></description></item><item><title>Azure SQL Database Export to Localhost using SqlPackage</title><link>https://byalexblog.net/article/azure-sql-database-export-to-local/</link><pubDate>Thu, 12 Sep 2024 06:53:27 +0000</pubDate><guid>https://byalexblog.net/article/azure-sql-database-export-to-local/</guid><description>&lt;img src="https://byalexblog.net/images/azure-sql-database-export-to-local/terminal.png" alt=""> Exporting an Azure SQL Server database locally is a common task for developers and administrators who need to create backups, test changes, or migrate databases to other environments. If your SQL Server instance is configured with Entra ID (formerly Azure AD) authentication, you might face an issue connecting to the database with username and password from the command line tool like &lt;a href="https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage?view=sql-server-ver16" target="_blank" rel="noopener">SqlPackage&lt;/a>.
In this guide, I’ll walk you through the process of exporting an Azure SQL Server database to your local windows machine using Powershell, SqlPackage tool and Azure SQL Server with Entra ID authentication.</description></item><item><title>Is Azure Resource Group just a Logical Container?</title><link>https://byalexblog.net/article/azure-resource-group/</link><pubDate>Thu, 11 Jul 2024 13:19:52 +0000</pubDate><guid>https://byalexblog.net/article/azure-resource-group/</guid><description>&lt;img src="https://byalexblog.net/images/azure-resource-group/logo.png" alt=""> I was always thinking about Azure Resource Groups as a Logical container for resources that helps in managing and organizing resources in Azure. But it appears that there are some limitations that I was not aware of.</description></item><item><title>Azure DevOps Terraform Pipeline</title><link>https://byalexblog.net/article/azure-devops-terraform-pipeline/</link><pubDate>Sat, 22 Jun 2024 07:38:40 +0000</pubDate><guid>https://byalexblog.net/article/azure-devops-terraform-pipeline/</guid><description>&lt;img src="https://byalexblog.net/images/azure-devops-terraform-pipeline/logo.png" alt=""> I was recently working on a project that required me to create a Terraform pipeline in Azure DevOps. I had never done this before, so I had to do some research to figure out how to set it up. In this article, I will share the final pipeline that I created, as well as some of the resources that I found helpful along the way.</description></item><item><title>Azure Function Encountered an Error ServiceUnavailable From Host Runtime</title><link>https://byalexblog.net/article/azure-function-encountered-an-error-serviceunavailable-from-host-runtime/</link><pubDate>Sat, 10 Feb 2024 10:03:16 +0000</pubDate><guid>https://byalexblog.net/article/azure-function-encountered-an-error-serviceunavailable-from-host-runtime/</guid><description>&lt;p>&lt;img src="https://byalexblog.net/images/azure-function-encountered-an-error-serviceunavailable-from-host-runtime/logo.webp" alt="">
I recently had an issue deploying Python 3.10 Azure Function on App Service Plan. The error message was:&lt;/p>
&lt;pre tabindex="0">&lt;code>9:38:43 AM myfunc: Deployment successful. deployer = ms-azuretools-vscode deploymentPath = Functions App ZipDeploy. Extract zip. Remote build.
9:38:56 AM myfunc: Syncing triggers...
9:39:50 AM myfunc: Syncing triggers (Attempt 2/6)...
9:40:51 AM myfunc: Syncing triggers (Attempt 3/6)...
9:42:01 AM myfunc: Syncing triggers (Attempt 4/6)...
9:43:01 AM myfunc: Syncing triggers (Attempt 5/6)...
9:45:12 AM myfunc: Syncing triggers (Attempt 6/6)...
9:45:33 AM: Error: Encountered an error (ServiceUnavailable) from host runtime.
&lt;/code>&lt;/pre>&lt;p>Here is my investigation and solution.&lt;/p></description></item><item><title>Rethinking Code Comments in the AI Era</title><link>https://byalexblog.net/article/rethinking-code-comments-in-the-ai-era/</link><pubDate>Sat, 27 Jan 2024 21:40:54 +0000</pubDate><guid>https://byalexblog.net/article/rethinking-code-comments-in-the-ai-era/</guid><description>&lt;img src="https://byalexblog.net/images/i-like-to-write-comments-now/logo.jpeg" alt=""> In the realm of software development, the utility of comments in code has long been a subject of debate. Traditionally, many have held the belief that high-quality code should speak for itself, rendering comments unnecessary. This viewpoint advocates for self-explanatory code through well-named variables, functions, and classes, and a logical assembly of the code structure. However, the advent of AI-assistants like GitHub Copilot is challenging this notion, ushering in a new perspective on the role of comments in coding. In this blog post, we&amp;rsquo;ll explore how AI is reshaping our approach to commenting, transforming it from a tedious task to an integral part of the coding process.</description></item><item><title>Azure API Management to Azure Cosmosdb</title><link>https://byalexblog.net/article/azure-apimanagement-to-azure-cosmosdb/</link><pubDate>Tue, 07 Nov 2023 20:12:52 +0000</pubDate><guid>https://byalexblog.net/article/azure-apimanagement-to-azure-cosmosdb/</guid><description>&lt;img src="https://byalexblog.net/images/azure-apimanagement-to-azure-cosmosdb/apim_cosmosdb_logo.jpeg" alt="">
In my previous posts I have showed you how to connect &lt;a href="https://byalexblog.net/article/azure-apimanagement-to-azure-service-bus/" target="_blank" rel="noopener">Azure API Management to Azure Service Bus&lt;/a> and &lt;a href="https://byalexblog.net/article/azure-apimanagement-to-azure-storage-account/" target="_blank" rel="noopener">Azure API Management to Azure Storage account&lt;/a>. In this post I will show you how to connect Azure API Management to Azure Cosmosdb.</description></item><item><title>Host React frontend app on Azure Storage Account</title><link>https://byalexblog.net/article/react-azure-storage/</link><pubDate>Sat, 19 Aug 2023 11:56:40 +0000</pubDate><guid>https://byalexblog.net/article/react-azure-storage/</guid><description>&lt;img src="https://byalexblog.net/images/react-azure-storage/react-azure-storage-logo.jpg" alt="">
Running a React frontend web application on an Azure Storage account is a great way to host your app with a low cost and high scalability. In this post, we&amp;rsquo;ll go through the steps of setting up a new Azure Storage account and deploying your React app to it.</description></item><item><title>Azure API Management to Azure Service Bus</title><link>https://byalexblog.net/article/azure-apimanagement-to-azure-service-bus/</link><pubDate>Sat, 08 Jul 2023 10:19:45 +0000</pubDate><guid>https://byalexblog.net/article/azure-apimanagement-to-azure-service-bus/</guid><description>&lt;img src="https://byalexblog.net/images/azure-apimanagement-to-azure-service-bus/apim_to_sb.jpg" alt="">
In my &lt;a href="https://byalexblog.net/article/azure-apimanagement-to-azure-storage-account/" title="Azure API Management to Azure Storage Account" target="_blank" rel="noopener">previous post&lt;/a> I showed how to use &lt;a href="https://azure.microsoft.com/en-us/products/api-management" title="Azure API Management" target="_blank" rel="noopener">Azure API Management&lt;/a> to expose an &lt;a href="https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview" title="Azure Storage Account" target="_blank" rel="noopener">Azure Storage Account&lt;/a>. In this post I will show how to use Azure API Management to expose an &lt;a href="https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview" title="Azure Service Bus" target="_blank" rel="noopener">Azure Service Bus&lt;/a>.
This combination is useful when you have a fire and forget HTTP endpoint and you expect irregular traffic. For example, you are designing a mobile application crash reporting system. You want to send the crash report to the server and forget about it. You don&amp;rsquo;t want to wait for the response. You don&amp;rsquo;t want to block the user interface. You don&amp;rsquo;t want to retry if the server is not available. It might happen that a new mobile app version has a significant bug and you get a lot of crash reports. In this case, it is reasonable to use Azure Service Bus to queue the crash reports for peak times and process them later.
As in my &lt;a href="https://byalexblog.net/article/azure-apimanagement-to-azure-storage-account/" title="Azure API Management to Azure Storage Account" target="_blank" rel="noopener">previous post&lt;/a>, I will use a scenario with two brands (Adidas and Nike) to show you the flexibility of the combination Azure API Management plus Azure Service Bus. I will also define everything in &lt;a href="https://www.terraform.io/" title="Terraform" target="_blank" rel="noopener">Terraform&lt;/a> so the solution deployment is fully automated.</description></item><item><title>Azure API Management to Azure Storage Account</title><link>https://byalexblog.net/article/azure-apimanagement-to-azure-storage-account/</link><pubDate>Thu, 06 Jul 2023 08:32:23 +0000</pubDate><guid>https://byalexblog.net/article/azure-apimanagement-to-azure-storage-account/</guid><description>&lt;img src="https://byalexblog.net/images/azure-apimanagement-to-azure-storage-account/apim_to_blob.jpg" alt=""> There can be the case when you need to upload a file or metadata to &lt;a href="https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview" target="_blank" rel="noopener">Azure Storage Account&lt;/a> from an application which is outside of your cloud infrastructure. You should not expose your storage account to the internet for that purpose. There are many reasons for that: securuty, flexibility, monitoring etc. The natural solution to acheave the goal is to use something in between of your Azure Storage Account and the public internet. Some kind of HTTP proxy that will manage user authentication, do some simple validations and then pass the request to Storage Account to save the request body data as a blob. &lt;a href="https://azure.microsoft.com/en-us/products/api-management" target="_blank" rel="noopener">Azure API Management&lt;/a> is a perfect candidate for that role. It is a fully managed service that means you don&amp;rsquo;t need to create any custom application. It is highly available and scalable. It has a lot of features that can be used for your needs.
In this article I will show how to configure Azure API Management to upload a file to Azure Storage Account.</description></item><item><title>Breaking Through Barriers: Simplifying CD with Automated Azure SQL Schema Changes!</title><link>https://byalexblog.net/article/azure-sql-schema-deployment/</link><pubDate>Sat, 10 Jun 2023 10:57:21 +0000</pubDate><guid>https://byalexblog.net/article/azure-sql-schema-deployment/</guid><description>&lt;img src="https://byalexblog.net/images/azure-sql-schema-deployment/logo.jpg" alt=""> Have you ever tried to deploy a Microsoft SQL database schema to Azure SQL using a linux-based CI/CD pipeline? If you did, you probably know that it is not a trivial task. The reason is that the &lt;a href="https://www.nuget.org/packages/Microsoft.Data.Tools.Msbuild" target="_blank" rel="noopener">Microsoft.Data.Tools.Msbuild&lt;/a> package is not available for linux. The package contains MSBuild targets and properties that are used to build and deploy database projects. The package is a part of &lt;a href="https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt?view=sql-server-ver15" target="_blank" rel="noopener">SQL Server Data Tools (SSDT)&lt;/a> and it is not available for linux. So if you want to deploy a database schema to Azure SQL using a linux-based CI/CD pipeline you need to use a different approach. In this article we will see how to deploy a Microsoft SQL database schema to Azure SQL using &lt;strong>any&lt;/strong> linux-based CI/CD pipeline (Gitlab, GitHub, Azure DevOps, etc).</description></item><item><title>Azure Function Serverless Deployment Python</title><link>https://byalexblog.net/article/azure-function-serverless-deployment-python/</link><pubDate>Fri, 26 May 2023 12:03:58 +0000</pubDate><guid>https://byalexblog.net/article/azure-function-serverless-deployment-python/</guid><description>&lt;img src="https://byalexblog.net/images/azure-function-serverless-deployment-python/logo.jpg" alt=""> Consumption plan is the cheapest way to run your Azure Function. However, it has some limitations. For example, you can not use Web Deploy, Docker Container, Source Control, FTP, Cloud sync or Local Git. You can use &lt;a href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-deployment-technologies#external-package-url" target="_blank" rel="noopener">External package URL&lt;/a> or &lt;a href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-deployment-technologies#zip-deploy" target="_blank" rel="noopener">Zip deploy&lt;/a> instead. In this article I will show you how to deploy Python Azure Function &lt;a href="https://techcommunity.microsoft.com/t5/azure-compute-blog/azure-functions-v2-python-programming-model/ba-p/3665168" target="_blank" rel="noopener">Programming Model v2&lt;/a> App to Linux Consumption Azure Function resource using Zip Deployment.</description></item><item><title>Azure Function Serverless Deployment Dotnet</title><link>https://byalexblog.net/article/azure-function-serverless-deployment-dotnet/</link><pubDate>Wed, 24 May 2023 18:53:32 +0000</pubDate><guid>https://byalexblog.net/article/azure-function-serverless-deployment-dotnet/</guid><description>&lt;img src="https://byalexblog.net/images/azure-function-serverless-deployment-dotnet/logo.jpg" alt=""> Consumption plan is the cheapest way to run your Azure Function. However, it has some limitations. For example, you can not use Web Deploy, Docker Container, Source Control, FTP, Cloud sync or Local Git. You can use &lt;a href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-deployment-technologies#external-package-url" target="_blank" rel="noopener">External package URL&lt;/a> or &lt;a href="https://learn.microsoft.com/en-us/azure/azure-functions/functions-deployment-technologies#zip-deploy" target="_blank" rel="noopener">Zip deploy&lt;/a> instead. In this article I will show you how to deploy Dotnet Isolated Azure Function App to Linux Consumption Azure Function resource using Zip Deployment.</description></item><item><title>Dapper queries synchronized with MSSQL database schema</title><link>https://byalexblog.net/article/dapper-queries-synchronized-with-mssql-database-schema/</link><pubDate>Thu, 25 Aug 2022 12:43:45 +0200</pubDate><guid>https://byalexblog.net/article/dapper-queries-synchronized-with-mssql-database-schema/</guid><description>&lt;img src="https://byalexblog.net/images/dapper-queries-synchronized-with-mssql-database-schema/logo.jpg" alt=""> Dapper is a MicroORM that allows you to control SQL queries you are executing and removes the pain of mapping the dataset results back to your domain model. The thing is that when you specify SQL queries you have to make sure they are valid against the current Database schema. One solution is to use Stored Procedures&amp;hellip; other one is this&amp;hellip;</description></item><item><title>AWS Certificate Manager TLS Certificate import</title><link>https://byalexblog.net/article/aws-certificatemanager-tls-cert-import/</link><pubDate>Tue, 23 Aug 2022 11:18:30 +0200</pubDate><guid>https://byalexblog.net/article/aws-certificatemanager-tls-cert-import/</guid><description>When you are going to use TLS certificate with AWS CloudFront, AWS Elastic Load Balancing, AWS API Gateway or other integrated services you can use that by importing the certificate data to the AWS Certificate Manager.</description></item><item><title>Deploying dotnet6 app on AWS ElasticBeanstalk Graviton2 with HTTPS termination on EC2 instance</title><link>https://byalexblog.net/article/dotnet6-elasticbeanstalk/</link><pubDate>Mon, 22 Aug 2022 10:20:06 +0200</pubDate><guid>https://byalexblog.net/article/dotnet6-elasticbeanstalk/</guid><description>On one of my project it was a requirement to reduce the ElasticBeanstalk hosting costs to minimum for dotnet application. The app is a kind of dashboard with limited audience and nothing heavy inside. I decided to use Graviton2 instances (ARM processors on board) in order to acheave best speed/cost ratio.</description></item><item><title>My current command to format hard drive for linux NAS</title><link>https://byalexblog.net/article/linux-nas-drive-format-command/</link><pubDate>Tue, 03 Aug 2021 21:22:05 +0300</pubDate><guid>https://byalexblog.net/article/linux-nas-drive-format-command/</guid><description>Put it here to just not forget =) Here is my command to format my hard drives for NAS. I came to this when I bought 2 10TB hard drives for Chia coin mining. &lt;code>bash sudo mkfs.ext4 /dev/sde1 -T largefile4 -m 0&lt;/code></description></item><item><title>Azure Api Management Automated Revisions</title><link>https://byalexblog.net/article/azure-api-management-automated-revisions/</link><pubDate>Wed, 28 Jul 2021 23:41:05 +0300</pubDate><guid>https://byalexblog.net/article/azure-api-management-automated-revisions/</guid><description>Powershell script to deploy new WebAPI and release new API Management revision with zero downtime.</description></item><item><title>Future for business analysts</title><link>https://byalexblog.net/article/future-for-business-analysts/</link><pubDate>Thu, 20 Oct 2016 12:12:40 +0300</pubDate><guid>https://byalexblog.net/article/future-for-business-analysts/</guid><description>Software products can be much better if anybody start asking questions about who will use it. We need user researchers called Interaction Designers (IxD) that will prove we are creating something that can be used by people. Where to find them? Business analysts are best people to start looking.</description></item><item><title>3 steps to create integration tests for your ASP.NET MVC 5 application</title><link>https://byalexblog.net/article/3-steps-integration-testing-aspnet-mvc/</link><pubDate>Sun, 02 Oct 2016 09:43:29 +0300</pubDate><guid>https://byalexblog.net/article/3-steps-integration-testing-aspnet-mvc/</guid><description>&lt;p>&lt;img src="https://byalexblog.net/images/3-steps-integration-testing-aspnet-mvc/integration-testing-mock-component.png" alt="">
When you develop an ASP.NET MVC application you should test it anyway. You can cover different parts of your application logic with unit tests or you can create tests that look like user interaction scenarios. These tests have several advantages over unit tests:&lt;/p>
&lt;ul>
&lt;li>These tests are independent of implementation and you can&amp;rsquo;t break your test by refactoring (the only scenario when you should modify your test is functional requirement change).&lt;/li>
&lt;li>These tests are good documentation for your application.&lt;/li>
&lt;li>These tests can give you idea about that your users need and how will they supposed to use concrete feature.&lt;/li>
&lt;/ul></description></item><item><title>Merge SQL databases</title><link>https://byalexblog.net/article/merge-sql-databases/</link><pubDate>Fri, 27 May 2016 11:55:25 +0300</pubDate><guid>https://byalexblog.net/article/merge-sql-databases/</guid><description>Imagine you have 2 databases with identical schema. These databases were working for different application instances for some time. And now you need to merge them together for some reason. You can use different tools to achieve this goal. For example &lt;a href="https://www.devart.com/ru/dbforge/sql/datacompare/" target="_blank" rel="noopener">dbForge Data Compare&lt;/a> or &lt;a href="http://www.red-gate.com/products/sql-development/sql-data-compare/" target="_blank" rel="noopener">SQL Data Compare&lt;/a>. But these tools cost money and if you don&amp;rsquo;t merge databases every day this is probably not an option for you. Also these tools does not know full specific of your database structure including unique indexes, check constraints and triggers. Another big deal is identity columns that are using as primary keys. For two databases these keys can be same but represent different entities.  In my practice I face database merge task second time and here is how I handle it.</description></item><item><title>RabbitMQ poisoned messages handling</title><link>https://byalexblog.net/article/rabbitmq-poisoned-messages-handeling/</link><pubDate>Tue, 09 Feb 2016 22:59:03 +0300</pubDate><guid>https://byalexblog.net/article/rabbitmq-poisoned-messages-handeling/</guid><description>You never know what will come from remote system. Even if you keep everything under control, there always can be something unexpected. In microservice architecture each component must be ready for everything.</description></item><item><title>Custom domain name for Azure VM</title><link>https://byalexblog.net/article/custom-domain-name-for-azure-vm/</link><pubDate>Sun, 04 Oct 2015 23:33:03 +0300</pubDate><guid>https://byalexblog.net/article/custom-domain-name-for-azure-vm/</guid><description>If you are running VM on Azure you know that your default domain name looks like this: &lt;code>yourservicename.cloudapp.net&lt;/code>. Also public IP address is not static that means you can&amp;rsquo;t use &amp;lsquo;A&amp;rsquo; DNS record on it.</description></item><item><title>Using MSBuild to deploy composite web application</title><link>https://byalexblog.net/article/using-msbuild-to-deploy-composite-web-application/</link><pubDate>Thu, 30 Apr 2015 17:30:52 +0300</pubDate><guid>https://byalexblog.net/article/using-msbuild-to-deploy-composite-web-application/</guid><description>How to deploy AngularJS + ASP.NET Web.API application using MSBuild</description></item><item><title>The biggest problem of all software developers</title><link>https://byalexblog.net/article/the-biggest-problem-of-all-software-developers/</link><pubDate>Sun, 29 Mar 2015 11:16:38 +0300</pubDate><guid>https://byalexblog.net/article/the-biggest-problem-of-all-software-developers/</guid><description>The biggest problem of all software developers</description></item><item><title>Reading user defined columns from database with Entity Framework</title><link>https://byalexblog.net/article/entity-framework-dynamic-columns/</link><pubDate>Thu, 12 Mar 2015 20:43:25 +0300</pubDate><guid>https://byalexblog.net/article/entity-framework-dynamic-columns/</guid><description>Reading user defined columns from database with Entity Framework</description></item></channel></rss>